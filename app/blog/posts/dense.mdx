---
title: DenseNets
publishedAt: 2024-11-08
summary: densenets vs residual learning.
image: 'https://images.fineartamerica.com/images/artworkimages/medium/1/space-image-carina-nebula-orange-red-blue-matthias-hauser.jpg'
---

> *[Original Paper](https://arxiv.org/abs/1608.06993)*

<br/>

<details><summary>TLDR</summary> Better Gradient Flow </details>

<br/>

**[Densely Connected Convolutional Neural Networks](https://arxiv.org/abs/1608.06993)**, at least when you don't have any skin in the game of Deep Learning, can seem to resemble **[residual networks](https://arxiv.org/pdf/1512.03385)**, with the exception that they have skip connections at every layer.
<br/>
Recall, a Residual Block (ResNet) at every $\ell * m$ layer, where $\ell * m < \mathcal{R}_L$ and $\mathcal{R}_L$ is the total count of layers, contains a skip connection, formulated as:

```math

H_{\ell} = \mathcal{R}(H_{\ell - 1}) + \mathcal{F}(H_{\ell - 1})

```

where $H_{\ell - 1}$ is the output of the previous layer, $\mathcal{R}$ is the transformation applied through the residual conenction, and $\mathcal{F}$ is the residual function.
<br/>
> In the case of a ConvNet, $\mathcal{F}$ is simply the convolution of the kernel for the $\ell$th layer, $\mathcal{K}_{\ell}$, with the input, in this case being $H_{\ell - 1}$. $\mathcal{K_{\ell}}$ learns the residual between $H_{\ell - 1}$ and the desired output, $H_{\ell}$.<br/> 
> <br/>
> Note that this is all learnt on the fly via Gradient Descent $\dots$ it's still pretty mindblowing to me.
<br/>
$\mathcal{R}$ can be formulated as an identity tranformation, $I$, or another transformation such as a $1 \times 1$ convolution or a gating function, as done in **[Highway Networks](https://arxiv.org/pdf/1505.00387)**, proposed by Srivastava et al.
<br/>
In the case of ResNets, in the paper *"Identity Mappings in Deep Residual Networks"* by He et al., it was found that the $I$ transformation, proved to be the superior transformation for residual connections, such that it allowed for better gradient flow during backpropagation in the Residual Network
<br/>
> See my earlier blog post, **[here](/blog/residuals)**, for more detail and interpretation.
<br/>

We'll refer to $\mathcal{R} = I$ as $\mathcal{R}_I$
<br/>
The improved gradient flow, via $\mathcal{R}_I$, was able to mitigate the problem of vanishing $∂'s$, by rescaling the $∂$ through an additional factor in the formulation of the chain rule, at least at each $\ell$ that contained $\mathcal{R}_I$.
<br/>
Though, ResNets are still limited, as they can only propagate $∂$'s back to the layers that specifically contain $\mathcal{R}_I$, which is typically about every $2$ or $3$ layers.
<br/>
DenseNets on the other hand, leverage skip connections at **every layer** within it's DenseBlock, $\mathcal{D}$, such that every single layer, $\ell \in \mathcal{D}$, has access to $∂$'s of the loss, $\mathcal{L}$, w.r.t to the outputs of **every** subsequent layer, $\ell + m$, where $\ell + m < \mathcal{D}_L$ and $\mathcal{D}_L$ is the total count of $\ell \in \mathcal{D}$.
<br/>
And these skip connections aren't the identity transformation, but simply a channel-wise concatenation of previous inputs to the output. The concatenation at every $\ell$ is what allows for the improved gradient flow.
<br/>
<div align = 'center'>
<img style = {{ width: '400px'}} src = 'https://pytorch.org/assets/images/densenet1.png'/>
<em style = {{fontSize: '10px'}}> DenseBlock! </em>
</div>
<br/>

## Gradient Flow $\in \mathcal{D}$
<br/>
The forward pass for a DenseBlock can be given as:

```math

H_{\ell} = f_{\ell}([H_0, H_1, ..., H_{\ell - 1}]) \tag{1}

```

where the argument passed into the current, $\ell th$ layer, is a concatenation of all previous outputs with the current input within the given DenseBlock.
<br/>
> Note that $\ell$ must be $0 < \ell < \mathcal{D}_L$ ($\mathcal{D}_L$ is total num of layers in the DenseBlock)
<br/>
To compute the gradient for any given $H_{\ell}$, you first can note that a given $H_{\ell}$, as given in the function above, depend on all $H_{\ell - n}$ (where $0 < n < \ell$).
<br/>
Also, $H_{\ell}$ contributes to all $H_{i + m}$ (where $0 < m < (L - \ell)$).

```math
H_{i + m} = f_{\ell}([H_0, ... , H_{\ell}, ..., H_{\ell + m - 1}]) \tag{2}
```
<br/>
Say we have a loss, $\mathcal{L}$, and we want to take the gradient of the loss w.r.t to $H_{\ell}$.

```math

\frac{∂\mathcal{L}}{∂H_{\ell}}

```
<br/>
> *This will be denoted as $∂H_{\ell}$ for simplicity*
<br/>
To compute $∂H_{\ell}$, we need to account for it's gradient $\forall \hspace{1mm} \ell \in L$, in which $H_{\ell}$ contributed to an output.
<br/>
> So using equation $(2)$, $\forall \hspace{1mm} m$
<br/>
Then the total gradient (accumulated across all $\ell \in L$, where $H_{\ell}$ had a contribution) is simply a summation of all $∂H_{\ell}$, $\forall \ell \in L$, where $H_{\ell}$ had a contribution.

```math
\text{EQ. (3)}\\[3mm]
\frac{∂\mathcal{L}}{∂H_{\ell}} = \sum_{j = \ell + i}^L (\frac{∂\mathcal{L}}{[∂H_{0}, ∂H_{1}, \dots, ∂H_{\ell}, \dots,∂H_{j-1}]})(\frac{[∂H_{0}, ∂H_{1}, \dots, ∂H_{\ell}, \dots,∂H_{j-1}]}{∂H_{\ell}})

```
<br/>
> $\ell$ is equal to the current layer. 
> <br/>
> We do $j = \ell + 1$ in the $\sum$ as $H_{\ell}$ only had a contribution in layers after $\ell$, and not $\ell$.
> <br/>
> We do $j - 1$ in the denominator of the first factor as if we did $H_L$, it wouldn't make mathematical sense as for the last layer of the DenseBlock ($L$), it's output can't contribute to itself.
<br/>
Taking a look at the overall gradent flow, for $\ell = 3$ in a 5-layer DenseBlock:

```math

\frac{∂\mathcal{L}}{∂H_{\ell}} = (\frac{∂\mathcal{L}}{∂H_{\ell + 2}})(\frac{∂H_{\ell+2}}{∂H_{\ell + 1}})(\frac{∂H_{\ell+1}}{∂H_{\ell}})

```

it's easy to see how the vanishing gradient can be diminished, as we're accumulating gradients for the respective $∂H_{\ell+2}, ∂H_{\ell+1}, ∂H_{\ell}$, through equation $3$, meaning $\forall \hspace{1mm} \ell \in \mathcal{D}_L$ that any given $H_{\ell}$ had a contribution, we include the respective $∂H_{\ell}$ in the $\sum$, leading to a higher magnitude of gradients, that then gets propagated back **again** to compute every remaining $∂H_{\ell - j}$, where $0 < j < \ell$.
<br/>
Albeit, given that we're accumulating gradients, a common conception might be the inverse, exploding gradients.
<br/>
BatchNormalization exists for mitigating this very issue, such that inputs to the $\text{ReLU}$, don't become so large that $∂$'s begin to explode as we backpropagate deeper, into earlier layers.
<br/>

## Diversified Training
<br/>
Coming alongside the improved gradient flow, via concatenation, the other benefit of DenseNet, over Residual Networks, include their ability to learn more diversified features at every layer.
<br/>
A given layer is being fed multiple sets of feature maps from previous layers, such that the $\mathcal{K}_{\ell}$ is able to extract correlated features across multiple previous convolutional layers $\in \mathcal{D}$.
<br/>
The complexity of which a DenseNet hierarchically concatenates previous outputs to the current input can be given by the growth rate, $k$, where $k$ is the total count of output channels for a given $\ell \in \mathcal{D}$.
<br/>
> *$\forall \ell \in \mathcal{D}$, the amount of channels to the input of layer $\ell$ grows by $k$.*
<br/>
The hyperparam $k$, is what allows you to easily control the complexity of the DenseNet, across it's entire architecture.
<br/>
*fin*
